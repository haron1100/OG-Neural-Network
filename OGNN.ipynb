{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0706, 0.0706, 0.0706,\n",
      "          0.4941, 0.5333, 0.6863, 0.1020, 0.6510, 1.0000, 0.9686, 0.4980,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.1176, 0.1412, 0.3686, 0.6039, 0.6667, 0.9922, 0.9922, 0.9922,\n",
      "          0.9922, 0.9922, 0.8824, 0.6745, 0.9922, 0.9490, 0.7647, 0.2510,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1922,\n",
      "          0.9333, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
      "          0.9922, 0.9843, 0.3647, 0.3216, 0.3216, 0.2196, 0.1529, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706,\n",
      "          0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765, 0.7137,\n",
      "          0.9686, 0.9451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.3137, 0.6118, 0.4196, 0.9922, 0.9922, 0.8039, 0.0431, 0.0000,\n",
      "          0.1686, 0.6039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0549, 0.0039, 0.6039, 0.9922, 0.3529, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.5451, 0.9922, 0.7451, 0.0078, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0431, 0.7451, 0.9922, 0.2745, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.1373, 0.9451, 0.8824, 0.6275,\n",
      "          0.4235, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3176, 0.9412, 0.9922,\n",
      "          0.9922, 0.4667, 0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1765, 0.7294,\n",
      "          0.9922, 0.9922, 0.5882, 0.1059, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0627,\n",
      "          0.3647, 0.9882, 0.9922, 0.7333, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.9765, 0.9922, 0.9765, 0.2510, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1804, 0.5098,\n",
      "          0.7176, 0.9922, 0.9922, 0.8118, 0.0078, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.1529, 0.5804, 0.8980, 0.9922,\n",
      "          0.9922, 0.9922, 0.9804, 0.7137, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0941, 0.4471, 0.8667, 0.9922, 0.9922, 0.9922,\n",
      "          0.9922, 0.7882, 0.3059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0902, 0.2588, 0.8353, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765,\n",
      "          0.3176, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.6706,\n",
      "          0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.7647, 0.3137, 0.0353,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.2157, 0.6745, 0.8863, 0.9922,\n",
      "          0.9922, 0.9922, 0.9922, 0.9569, 0.5216, 0.0431, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.5333, 0.9922, 0.9922, 0.9922,\n",
      "          0.8314, 0.5294, 0.5176, 0.0627, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "Label: tensor(5)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# GET THE TRAINING DATASET\n",
    "train_data = datasets.MNIST(\n",
    "    root='MNIST-data',                        # where is the data (going to be) stored\n",
    "    transform=transforms.ToTensor(),          # transform the data from a PIL image to a tensor\n",
    "    train=True,                               # is this training data?\n",
    "    download=True                             # should i download it if it's not already here?\n",
    ")\n",
    "\n",
    "# GET THE TEST DATASET\n",
    "test_data = datasets.MNIST(\n",
    "    root='MNIST-data', # where is the data (going to be) stored\n",
    "    transform=transforms.ToTensor(), # transform the data from a PIL image to a tensor\n",
    "    train=False,    # this is not the training set\n",
    ")\n",
    "\n",
    "# PRINT THEIR LENGTHS AND VISUALISE AN EXAMPLE\n",
    "ex = train_data[0] # get the first example\n",
    "x = ex[0] # get the features (actual tensor data -the first thing in the example)\n",
    "y = ex[1] # get the labels (second thing in the example)\n",
    "print('Features:', x)\n",
    "print('Label:', y)\n",
    "t = transforms.ToPILImage() # create the transform that can be called to convert the tensor into a PIL Image\n",
    "img = t(x)    # call the transform on the tensor\n",
    "#img.show()    # show the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FURTHER SPLIT THE TRAINING INTO TRAINING AND VALIDATION\n",
    "train_data, val_data = torch.utils.data.random_split(train_data, [50000, 10000])    # split into 50K training & 10K validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "# MAKE TRAINING DATALOADER\n",
    "train_loader = torch.utils.data.DataLoader( # create a data loader\n",
    "    train_data, # what dataset should it sample from?\n",
    "    shuffle=True, # should it shuffle the examples?\n",
    "    batch_size=batch_size # how large should the batches that it samples be?\n",
    ")\n",
    "\n",
    "# MAKE VALIDATION DATALOADER\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_data,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# MAKE TEST DATALOADER\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class OGNN(torch.nn.Module): # create a neural network class\n",
    "    def __init__(self, originators=2): # initialiser\n",
    "        super().__init__() # initialise the parent class\n",
    "        self.l1_originators = torch.nn.ModuleList([torch.nn.Linear(784, 1024) for i in range(originators)])\n",
    "        self.l2_originators = torch.nn.ModuleList([torch.nn.Linear(1024, 256) for i in range(originators)])\n",
    "        self.l3_originators = torch.nn.ModuleList([torch.nn.Linear(256, 10) for i in range(originators)])\n",
    "        \n",
    "    def forward(self, x): # define the forward pass\n",
    "        x = x.view(-1, 784) # flatten out our image features into vectors\n",
    "        x = F.relu(random.choice(self.l1_originators)(x)) # pass through the first linear layer\n",
    "        x = F.relu(random.choice(self.l2_originators)(x)) # pass through the first linear layer\n",
    "        x = F.softmax(random.choice(self.l3_originators)(x), dim=1) # pass through the first linear layer\n",
    "        return x # return output\n",
    "    \n",
    "    #def predict(self, x):\n",
    "\n",
    "class NN(torch.nn.Module): # create a neural network class\n",
    "    def __init__(self): # initialiser\n",
    "        super().__init__() # initialise the parent class\n",
    "        self.layer1 = torch.nn.Linear(784, 1024) # create our first linear layer\n",
    "        self.layer2 = torch.nn.Linear(1024, 256) # create our second linear layer\n",
    "        self.layer3 = torch.nn.Linear(256, 10) # create our third linear layer\n",
    "        \n",
    "    def forward(self, x): # define the forward pass\n",
    "        x = x.view(-1, 784) # flatten out our image features into vectors\n",
    "        x = self.layer1(x) # pass through the first linear layer\n",
    "        x = F.relu(x) # apply activation function\n",
    "        x = self.layer2(x) # pass through the second linear layer\n",
    "        x = F.relu(x) # apply activation function\n",
    "        x = self.layer3(x) # pass through the third linear layer\n",
    "        x = F.softmax(x) # apply activation function\n",
    "        return x # return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0008, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.mse_loss(my_nn.l1_originators[0].weight, my_nn.l1_originators[1].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tBatch: 0 \tLoss: tensor(2.3025, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 1 \tLoss: tensor(2.3029, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 2 \tLoss: tensor(2.3024, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 3 \tLoss: tensor(2.2833, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 4 \tLoss: tensor(2.3016, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 5 \tLoss: tensor(2.3022, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 6 \tLoss: tensor(2.3018, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 7 \tLoss: tensor(2.2919, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 8 \tLoss: tensor(2.2787, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 9 \tLoss: tensor(2.2845, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 10 \tLoss: tensor(2.2683, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 11 \tLoss: tensor(2.2779, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 12 \tLoss: tensor(2.2083, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 13 \tLoss: tensor(2.2283, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 14 \tLoss: tensor(2.1539, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 15 \tLoss: tensor(2.1969, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 16 \tLoss: tensor(2.2253, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 17 \tLoss: tensor(2.1790, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 18 \tLoss: tensor(2.1319, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 19 \tLoss: tensor(2.1937, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 20 \tLoss: tensor(2.1018, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 21 \tLoss: tensor(2.0730, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 22 \tLoss: tensor(2.0594, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 23 \tLoss: tensor(2.0250, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 24 \tLoss: tensor(2.1020, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 25 \tLoss: tensor(2.1703, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 26 \tLoss: tensor(1.9194, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 27 \tLoss: tensor(2.0640, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 28 \tLoss: tensor(2.0340, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 29 \tLoss: tensor(2.0031, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 30 \tLoss: tensor(1.9052, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 31 \tLoss: tensor(1.9613, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 32 \tLoss: tensor(1.9289, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 33 \tLoss: tensor(1.8599, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 34 \tLoss: tensor(1.9254, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 35 \tLoss: tensor(1.8037, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 36 \tLoss: tensor(1.8781, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 37 \tLoss: tensor(1.8651, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 38 \tLoss: tensor(1.8425, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 39 \tLoss: tensor(1.7923, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 40 \tLoss: tensor(2.0352, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 41 \tLoss: tensor(1.9525, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 42 \tLoss: tensor(1.8313, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 43 \tLoss: tensor(1.7748, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 44 \tLoss: tensor(1.8725, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 45 \tLoss: tensor(1.8324, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 46 \tLoss: tensor(1.8443, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 47 \tLoss: tensor(1.8452, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 48 \tLoss: tensor(1.8042, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 49 \tLoss: tensor(1.7748, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 50 \tLoss: tensor(1.8753, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 51 \tLoss: tensor(1.7896, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 52 \tLoss: tensor(1.6647, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 53 \tLoss: tensor(1.9438, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 54 \tLoss: tensor(1.7909, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 55 \tLoss: tensor(1.7844, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 56 \tLoss: tensor(1.8110, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 57 \tLoss: tensor(1.6707, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 58 \tLoss: tensor(1.8269, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 59 \tLoss: tensor(1.8189, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 60 \tLoss: tensor(1.8477, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 61 \tLoss: tensor(1.7787, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 62 \tLoss: tensor(1.7981, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 63 \tLoss: tensor(1.7699, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 64 \tLoss: tensor(1.7633, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 65 \tLoss: tensor(1.7138, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 66 \tLoss: tensor(1.7172, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 67 \tLoss: tensor(1.7550, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 68 \tLoss: tensor(1.7315, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 69 \tLoss: tensor(1.7333, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 70 \tLoss: tensor(1.7551, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 71 \tLoss: tensor(1.7335, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 72 \tLoss: tensor(1.7351, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 73 \tLoss: tensor(1.7603, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 74 \tLoss: tensor(1.6892, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 75 \tLoss: tensor(1.7210, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 76 \tLoss: tensor(1.7362, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 77 \tLoss: tensor(1.7184, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 78 \tLoss: tensor(1.6971, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 79 \tLoss: tensor(1.6978, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 80 \tLoss: tensor(1.7200, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 81 \tLoss: tensor(1.6672, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 82 \tLoss: tensor(1.6668, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 83 \tLoss: tensor(1.6783, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 84 \tLoss: tensor(1.6446, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 85 \tLoss: tensor(1.6370, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 86 \tLoss: tensor(1.6229, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 87 \tLoss: tensor(1.7223, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 88 \tLoss: tensor(1.6637, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 89 \tLoss: tensor(1.6024, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 90 \tLoss: tensor(1.6848, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 91 \tLoss: tensor(1.8184, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 92 \tLoss: tensor(1.7862, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 93 \tLoss: tensor(1.6436, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 94 \tLoss: tensor(1.6366, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 95 \tLoss: tensor(1.7475, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 96 \tLoss: tensor(1.7333, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 97 \tLoss: tensor(1.7245, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 98 \tLoss: tensor(1.6983, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 99 \tLoss: tensor(1.6294, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 100 \tLoss: tensor(1.6188, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 101 \tLoss: tensor(1.6668, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 102 \tLoss: tensor(1.6634, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 103 \tLoss: tensor(1.7496, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 104 \tLoss: tensor(1.6731, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 105 \tLoss: tensor(1.6894, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 106 \tLoss: tensor(1.7202, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 107 \tLoss: tensor(1.6212, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 108 \tLoss: tensor(1.6729, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 109 \tLoss: tensor(1.7018, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 110 \tLoss: tensor(1.6002, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 111 \tLoss: tensor(1.6773, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 112 \tLoss: tensor(1.6520, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 113 \tLoss: tensor(1.6752, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 114 \tLoss: tensor(1.6333, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 115 \tLoss: tensor(1.6194, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 116 \tLoss: tensor(1.6773, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 117 \tLoss: tensor(1.6723, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 118 \tLoss: tensor(1.6484, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tBatch: 119 \tLoss: tensor(1.6277, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 120 \tLoss: tensor(1.6323, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 121 \tLoss: tensor(1.6460, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 122 \tLoss: tensor(1.6888, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 123 \tLoss: tensor(1.6777, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 124 \tLoss: tensor(1.6349, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 125 \tLoss: tensor(1.6658, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 126 \tLoss: tensor(1.6607, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 127 \tLoss: tensor(1.5923, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 128 \tLoss: tensor(1.6190, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 129 \tLoss: tensor(1.6185, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 130 \tLoss: tensor(1.6441, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 131 \tLoss: tensor(1.6542, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 132 \tLoss: tensor(1.5930, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 133 \tLoss: tensor(1.6986, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 134 \tLoss: tensor(1.6185, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 135 \tLoss: tensor(1.6036, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 136 \tLoss: tensor(1.6400, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 137 \tLoss: tensor(1.5876, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 138 \tLoss: tensor(1.5756, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 139 \tLoss: tensor(1.6594, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 140 \tLoss: tensor(1.7224, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 141 \tLoss: tensor(1.6467, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 142 \tLoss: tensor(1.6347, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 143 \tLoss: tensor(1.6256, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 144 \tLoss: tensor(1.6477, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 145 \tLoss: tensor(1.6410, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 146 \tLoss: tensor(1.6148, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 147 \tLoss: tensor(1.6069, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 148 \tLoss: tensor(1.6672, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 149 \tLoss: tensor(1.6460, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 150 \tLoss: tensor(1.6243, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 151 \tLoss: tensor(1.6141, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 152 \tLoss: tensor(1.5835, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 153 \tLoss: tensor(1.5965, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 154 \tLoss: tensor(1.6039, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 155 \tLoss: tensor(1.6250, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 156 \tLoss: tensor(1.6247, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 157 \tLoss: tensor(1.5860, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 158 \tLoss: tensor(1.5626, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 159 \tLoss: tensor(1.5764, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 160 \tLoss: tensor(1.5993, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 161 \tLoss: tensor(1.6087, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 162 \tLoss: tensor(1.5825, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 163 \tLoss: tensor(1.6095, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 164 \tLoss: tensor(1.6360, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 165 \tLoss: tensor(1.6417, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 166 \tLoss: tensor(1.6019, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 167 \tLoss: tensor(1.6173, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 168 \tLoss: tensor(1.5532, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 169 \tLoss: tensor(1.5674, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 170 \tLoss: tensor(1.5578, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 171 \tLoss: tensor(1.5896, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 172 \tLoss: tensor(1.5788, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 173 \tLoss: tensor(1.5582, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 174 \tLoss: tensor(1.6115, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 175 \tLoss: tensor(1.5886, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 176 \tLoss: tensor(1.5644, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 177 \tLoss: tensor(1.6168, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 178 \tLoss: tensor(1.5876, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 179 \tLoss: tensor(1.5926, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 180 \tLoss: tensor(1.5597, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 181 \tLoss: tensor(1.5873, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 182 \tLoss: tensor(1.5447, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 183 \tLoss: tensor(1.5422, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 184 \tLoss: tensor(1.5621, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 185 \tLoss: tensor(1.5760, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 186 \tLoss: tensor(1.5684, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 187 \tLoss: tensor(1.5586, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 188 \tLoss: tensor(1.6062, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 189 \tLoss: tensor(1.5797, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 190 \tLoss: tensor(1.5398, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 191 \tLoss: tensor(1.5732, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 192 \tLoss: tensor(1.5612, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 193 \tLoss: tensor(1.5751, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 194 \tLoss: tensor(1.6091, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 195 \tLoss: tensor(1.5651, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 0 \tLoss: tensor(1.5956, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 1 \tLoss: tensor(1.5971, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 2 \tLoss: tensor(1.5490, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 3 \tLoss: tensor(1.5336, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 4 \tLoss: tensor(1.5848, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 5 \tLoss: tensor(1.5704, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 6 \tLoss: tensor(1.5909, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 7 \tLoss: tensor(1.5473, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 8 \tLoss: tensor(1.5717, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 9 \tLoss: tensor(1.5767, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 10 \tLoss: tensor(1.5353, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 11 \tLoss: tensor(1.5627, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 12 \tLoss: tensor(1.5790, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 13 \tLoss: tensor(1.5790, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 14 \tLoss: tensor(1.5672, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 15 \tLoss: tensor(1.5507, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 16 \tLoss: tensor(1.5730, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 17 \tLoss: tensor(1.5718, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 18 \tLoss: tensor(1.5323, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 19 \tLoss: tensor(1.5517, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 20 \tLoss: tensor(1.5822, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 21 \tLoss: tensor(1.5502, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 22 \tLoss: tensor(1.5750, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 23 \tLoss: tensor(1.6113, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 24 \tLoss: tensor(1.5536, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 25 \tLoss: tensor(1.5683, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 26 \tLoss: tensor(1.5481, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 27 \tLoss: tensor(1.5644, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 28 \tLoss: tensor(1.5529, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 29 \tLoss: tensor(1.5527, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 30 \tLoss: tensor(1.5719, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 31 \tLoss: tensor(1.5758, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 32 \tLoss: tensor(1.5654, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 33 \tLoss: tensor(1.5585, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 34 \tLoss: tensor(1.5657, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 35 \tLoss: tensor(1.5783, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 36 \tLoss: tensor(1.5668, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 37 \tLoss: tensor(1.5782, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 38 \tLoss: tensor(1.5283, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 39 \tLoss: tensor(1.5438, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 40 \tLoss: tensor(1.5582, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 41 \tLoss: tensor(1.5525, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 42 \tLoss: tensor(1.5778, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 43 \tLoss: tensor(1.5687, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tBatch: 44 \tLoss: tensor(1.5553, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 45 \tLoss: tensor(1.5710, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 46 \tLoss: tensor(1.5942, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 47 \tLoss: tensor(1.5700, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 48 \tLoss: tensor(1.5643, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 49 \tLoss: tensor(1.5875, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 50 \tLoss: tensor(1.5323, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 51 \tLoss: tensor(1.5974, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 52 \tLoss: tensor(1.5416, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 53 \tLoss: tensor(1.5768, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 54 \tLoss: tensor(1.5298, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 55 \tLoss: tensor(1.5782, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 56 \tLoss: tensor(1.5319, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 57 \tLoss: tensor(1.5588, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 58 \tLoss: tensor(1.5557, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 59 \tLoss: tensor(1.5745, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 60 \tLoss: tensor(1.5479, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 61 \tLoss: tensor(1.5692, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 62 \tLoss: tensor(1.5929, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 63 \tLoss: tensor(1.5319, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 64 \tLoss: tensor(1.6108, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 65 \tLoss: tensor(1.5367, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 66 \tLoss: tensor(1.5380, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 67 \tLoss: tensor(1.5677, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 68 \tLoss: tensor(1.5749, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 69 \tLoss: tensor(1.5490, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 70 \tLoss: tensor(1.5621, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 71 \tLoss: tensor(1.5497, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 72 \tLoss: tensor(1.5489, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 73 \tLoss: tensor(1.5447, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 74 \tLoss: tensor(1.5657, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 75 \tLoss: tensor(1.5800, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 76 \tLoss: tensor(1.5461, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 77 \tLoss: tensor(1.5369, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 78 \tLoss: tensor(1.5619, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 79 \tLoss: tensor(1.5404, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 80 \tLoss: tensor(1.5629, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 81 \tLoss: tensor(1.5771, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 82 \tLoss: tensor(1.5730, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 83 \tLoss: tensor(1.5454, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 84 \tLoss: tensor(1.5593, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 85 \tLoss: tensor(1.5475, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 86 \tLoss: tensor(1.5598, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 87 \tLoss: tensor(1.5519, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 88 \tLoss: tensor(1.5631, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 89 \tLoss: tensor(1.5760, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 90 \tLoss: tensor(1.5378, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 91 \tLoss: tensor(1.5501, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 92 \tLoss: tensor(1.5541, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 93 \tLoss: tensor(1.5483, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 94 \tLoss: tensor(1.5618, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 95 \tLoss: tensor(1.5485, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 96 \tLoss: tensor(1.5638, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 97 \tLoss: tensor(1.5573, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 98 \tLoss: tensor(1.5640, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 99 \tLoss: tensor(1.5450, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 100 \tLoss: tensor(1.5364, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 101 \tLoss: tensor(1.5670, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 102 \tLoss: tensor(1.5460, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 103 \tLoss: tensor(1.5673, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 104 \tLoss: tensor(1.5414, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 105 \tLoss: tensor(1.5278, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 106 \tLoss: tensor(1.5500, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 107 \tLoss: tensor(1.5240, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 108 \tLoss: tensor(1.5431, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 109 \tLoss: tensor(1.5295, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 110 \tLoss: tensor(1.5250, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 111 \tLoss: tensor(1.5461, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 112 \tLoss: tensor(1.5731, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 113 \tLoss: tensor(1.5242, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 114 \tLoss: tensor(1.5454, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 115 \tLoss: tensor(1.5277, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 116 \tLoss: tensor(1.5620, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 117 \tLoss: tensor(1.5554, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 118 \tLoss: tensor(1.5356, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 119 \tLoss: tensor(1.5305, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 120 \tLoss: tensor(1.5649, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 121 \tLoss: tensor(1.5461, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 122 \tLoss: tensor(1.5255, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 123 \tLoss: tensor(1.5509, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 124 \tLoss: tensor(1.5458, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 125 \tLoss: tensor(1.5481, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 126 \tLoss: tensor(1.5349, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 127 \tLoss: tensor(1.5427, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 128 \tLoss: tensor(1.5424, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 129 \tLoss: tensor(1.5486, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 130 \tLoss: tensor(1.5580, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 131 \tLoss: tensor(1.5254, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 132 \tLoss: tensor(1.5600, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 133 \tLoss: tensor(1.5552, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 134 \tLoss: tensor(1.5281, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 135 \tLoss: tensor(1.5567, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 136 \tLoss: tensor(1.5386, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 137 \tLoss: tensor(1.5284, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 138 \tLoss: tensor(1.5339, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 139 \tLoss: tensor(1.5322, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 140 \tLoss: tensor(1.5993, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 141 \tLoss: tensor(1.5182, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 142 \tLoss: tensor(1.5433, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 143 \tLoss: tensor(1.5243, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 144 \tLoss: tensor(1.5199, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 145 \tLoss: tensor(1.5585, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 146 \tLoss: tensor(1.5228, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 147 \tLoss: tensor(1.5682, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 148 \tLoss: tensor(1.5496, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 149 \tLoss: tensor(1.5453, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 150 \tLoss: tensor(1.5467, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 151 \tLoss: tensor(1.5649, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 152 \tLoss: tensor(1.5517, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 153 \tLoss: tensor(1.5373, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 154 \tLoss: tensor(1.5354, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 155 \tLoss: tensor(1.5346, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 156 \tLoss: tensor(1.5543, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 157 \tLoss: tensor(1.5101, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 158 \tLoss: tensor(1.5591, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 159 \tLoss: tensor(1.5497, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 160 \tLoss: tensor(1.5259, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 161 \tLoss: tensor(1.5368, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 162 \tLoss: tensor(1.6134, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 163 \tLoss: tensor(1.5300, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 164 \tLoss: tensor(1.5622, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 165 \tLoss: tensor(1.5462, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 166 \tLoss: tensor(1.5440, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tBatch: 167 \tLoss: tensor(1.5672, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 168 \tLoss: tensor(1.5294, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 169 \tLoss: tensor(1.5788, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 170 \tLoss: tensor(1.5609, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 171 \tLoss: tensor(1.5339, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 172 \tLoss: tensor(1.5299, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 173 \tLoss: tensor(1.5623, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 174 \tLoss: tensor(1.5682, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 175 \tLoss: tensor(1.5260, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 176 \tLoss: tensor(1.5423, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 177 \tLoss: tensor(1.5470, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 178 \tLoss: tensor(1.5481, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 179 \tLoss: tensor(1.5499, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 180 \tLoss: tensor(1.5316, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 181 \tLoss: tensor(1.5361, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 182 \tLoss: tensor(1.5425, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 183 \tLoss: tensor(1.5410, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 184 \tLoss: tensor(1.5672, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 185 \tLoss: tensor(1.6051, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 186 \tLoss: tensor(1.5521, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 187 \tLoss: tensor(1.5523, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 188 \tLoss: tensor(1.5232, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 189 \tLoss: tensor(1.5563, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 190 \tLoss: tensor(1.5119, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 191 \tLoss: tensor(1.5401, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 192 \tLoss: tensor(1.5448, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 193 \tLoss: tensor(1.5690, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 194 \tLoss: tensor(1.5739, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 195 \tLoss: tensor(1.5445, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 0 \tLoss: tensor(1.5757, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 1 \tLoss: tensor(1.5625, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 2 \tLoss: tensor(1.5401, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 3 \tLoss: tensor(1.5304, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 4 \tLoss: tensor(1.5211, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 5 \tLoss: tensor(1.5585, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 6 \tLoss: tensor(1.5608, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 7 \tLoss: tensor(1.5534, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 8 \tLoss: tensor(1.5288, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 9 \tLoss: tensor(1.5202, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 10 \tLoss: tensor(1.5629, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 11 \tLoss: tensor(1.5567, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 12 \tLoss: tensor(1.5183, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 13 \tLoss: tensor(1.5467, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 14 \tLoss: tensor(1.5273, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 15 \tLoss: tensor(1.5461, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 16 \tLoss: tensor(1.5221, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 17 \tLoss: tensor(1.5534, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 18 \tLoss: tensor(1.5580, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 19 \tLoss: tensor(1.5358, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 20 \tLoss: tensor(1.5164, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 21 \tLoss: tensor(1.5400, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 22 \tLoss: tensor(1.5378, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 23 \tLoss: tensor(1.5349, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 24 \tLoss: tensor(1.4927, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 25 \tLoss: tensor(1.5444, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 26 \tLoss: tensor(1.5561, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 27 \tLoss: tensor(1.5502, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 28 \tLoss: tensor(1.5327, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 29 \tLoss: tensor(1.5261, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 30 \tLoss: tensor(1.5339, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 31 \tLoss: tensor(1.5329, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 32 \tLoss: tensor(1.5260, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 33 \tLoss: tensor(1.5349, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 34 \tLoss: tensor(1.5476, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 35 \tLoss: tensor(1.5330, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 36 \tLoss: tensor(1.5380, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 37 \tLoss: tensor(1.5078, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 38 \tLoss: tensor(1.5200, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 39 \tLoss: tensor(1.4978, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 40 \tLoss: tensor(1.5306, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 41 \tLoss: tensor(1.5283, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 42 \tLoss: tensor(1.5177, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 43 \tLoss: tensor(1.5412, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 44 \tLoss: tensor(1.5224, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 45 \tLoss: tensor(1.5366, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 46 \tLoss: tensor(1.5265, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 47 \tLoss: tensor(1.5133, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 48 \tLoss: tensor(1.5254, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 49 \tLoss: tensor(1.5364, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 50 \tLoss: tensor(1.5369, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 51 \tLoss: tensor(1.5105, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 52 \tLoss: tensor(1.5280, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 53 \tLoss: tensor(1.5268, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 54 \tLoss: tensor(1.5396, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 55 \tLoss: tensor(1.5563, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 56 \tLoss: tensor(1.5045, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 57 \tLoss: tensor(1.5309, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 58 \tLoss: tensor(1.4974, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 59 \tLoss: tensor(1.5541, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 60 \tLoss: tensor(1.5288, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 61 \tLoss: tensor(1.5264, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 62 \tLoss: tensor(1.5270, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 63 \tLoss: tensor(1.5343, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 64 \tLoss: tensor(1.5160, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 65 \tLoss: tensor(1.5466, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 66 \tLoss: tensor(1.5435, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 67 \tLoss: tensor(1.5381, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 68 \tLoss: tensor(1.5159, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 69 \tLoss: tensor(1.5222, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 70 \tLoss: tensor(1.5128, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 71 \tLoss: tensor(1.5328, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 72 \tLoss: tensor(1.5120, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 73 \tLoss: tensor(1.5375, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 74 \tLoss: tensor(1.5455, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 75 \tLoss: tensor(1.5260, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 76 \tLoss: tensor(1.5359, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 77 \tLoss: tensor(1.5223, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 78 \tLoss: tensor(1.5223, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 79 \tLoss: tensor(1.5387, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 80 \tLoss: tensor(1.5269, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 81 \tLoss: tensor(1.5113, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 82 \tLoss: tensor(1.5234, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 83 \tLoss: tensor(1.5422, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 84 \tLoss: tensor(1.5197, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 85 \tLoss: tensor(1.5169, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 86 \tLoss: tensor(1.4943, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 87 \tLoss: tensor(1.5052, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 88 \tLoss: tensor(1.5346, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 89 \tLoss: tensor(1.5351, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 90 \tLoss: tensor(1.5588, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 91 \tLoss: tensor(1.5341, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 92 \tLoss: tensor(1.5004, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 \tBatch: 93 \tLoss: tensor(1.5264, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 94 \tLoss: tensor(1.5254, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 95 \tLoss: tensor(1.5022, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 96 \tLoss: tensor(1.5175, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 97 \tLoss: tensor(1.5435, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 98 \tLoss: tensor(1.5301, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 99 \tLoss: tensor(1.5344, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 100 \tLoss: tensor(1.5151, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 101 \tLoss: tensor(1.5215, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 102 \tLoss: tensor(1.5281, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 103 \tLoss: tensor(1.5430, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 104 \tLoss: tensor(1.5123, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 105 \tLoss: tensor(1.5556, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 106 \tLoss: tensor(1.5593, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 107 \tLoss: tensor(1.5057, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 108 \tLoss: tensor(1.5250, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 109 \tLoss: tensor(1.5064, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 110 \tLoss: tensor(1.5158, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 111 \tLoss: tensor(1.5588, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 112 \tLoss: tensor(1.5313, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 113 \tLoss: tensor(1.5296, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 114 \tLoss: tensor(1.5387, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 115 \tLoss: tensor(1.5148, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 116 \tLoss: tensor(1.5498, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 117 \tLoss: tensor(1.5235, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 118 \tLoss: tensor(1.5113, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 119 \tLoss: tensor(1.5204, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 120 \tLoss: tensor(1.5252, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 121 \tLoss: tensor(1.5465, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 122 \tLoss: tensor(1.5375, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 123 \tLoss: tensor(1.5248, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 124 \tLoss: tensor(1.5153, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 125 \tLoss: tensor(1.5465, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 126 \tLoss: tensor(1.5108, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 127 \tLoss: tensor(1.5363, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 128 \tLoss: tensor(1.5354, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 129 \tLoss: tensor(1.5091, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 130 \tLoss: tensor(1.5166, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 131 \tLoss: tensor(1.5299, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 132 \tLoss: tensor(1.5246, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 133 \tLoss: tensor(1.5700, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 134 \tLoss: tensor(1.5522, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 135 \tLoss: tensor(1.5520, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 136 \tLoss: tensor(1.5592, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 137 \tLoss: tensor(1.5295, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 138 \tLoss: tensor(1.4892, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 139 \tLoss: tensor(1.5042, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 140 \tLoss: tensor(1.5365, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 141 \tLoss: tensor(1.5224, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 142 \tLoss: tensor(1.5221, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 143 \tLoss: tensor(1.5170, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 144 \tLoss: tensor(1.5245, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 145 \tLoss: tensor(1.5351, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 146 \tLoss: tensor(1.5154, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 147 \tLoss: tensor(1.5058, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 148 \tLoss: tensor(1.5233, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 149 \tLoss: tensor(1.5267, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 150 \tLoss: tensor(1.5077, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 151 \tLoss: tensor(1.5219, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 152 \tLoss: tensor(1.5301, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 153 \tLoss: tensor(1.5308, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 154 \tLoss: tensor(1.5447, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 155 \tLoss: tensor(1.5194, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 156 \tLoss: tensor(1.5225, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 157 \tLoss: tensor(1.5209, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 158 \tLoss: tensor(1.5387, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 159 \tLoss: tensor(1.5087, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 160 \tLoss: tensor(1.4834, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 161 \tLoss: tensor(1.5340, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 162 \tLoss: tensor(1.5482, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 163 \tLoss: tensor(1.5460, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 164 \tLoss: tensor(1.5519, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 165 \tLoss: tensor(1.5250, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 166 \tLoss: tensor(1.5073, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 167 \tLoss: tensor(1.5218, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 168 \tLoss: tensor(1.4989, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 169 \tLoss: tensor(1.5379, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 170 \tLoss: tensor(1.5180, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 171 \tLoss: tensor(1.5304, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 172 \tLoss: tensor(1.5433, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 173 \tLoss: tensor(1.5026, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 174 \tLoss: tensor(1.5060, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 175 \tLoss: tensor(1.5231, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 176 \tLoss: tensor(1.4987, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 177 \tLoss: tensor(1.5295, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 178 \tLoss: tensor(1.5499, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 179 \tLoss: tensor(1.5181, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 180 \tLoss: tensor(1.5541, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 181 \tLoss: tensor(1.5455, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 182 \tLoss: tensor(1.5342, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 183 \tLoss: tensor(1.5012, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 184 \tLoss: tensor(1.5265, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 185 \tLoss: tensor(1.5377, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 186 \tLoss: tensor(1.5212, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 187 \tLoss: tensor(1.5420, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 188 \tLoss: tensor(1.5353, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 189 \tLoss: tensor(1.5451, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 190 \tLoss: tensor(1.5291, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 191 \tLoss: tensor(1.5391, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 192 \tLoss: tensor(1.5436, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 193 \tLoss: tensor(1.5146, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 194 \tLoss: tensor(1.5431, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 195 \tLoss: tensor(1.5010, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 0 \tLoss: tensor(1.5145, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 1 \tLoss: tensor(1.5410, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 2 \tLoss: tensor(1.5072, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 3 \tLoss: tensor(1.5103, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 4 \tLoss: tensor(1.5238, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 5 \tLoss: tensor(1.5297, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 6 \tLoss: tensor(1.5021, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 7 \tLoss: tensor(1.5138, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 8 \tLoss: tensor(1.5193, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 9 \tLoss: tensor(1.5155, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 10 \tLoss: tensor(1.5031, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 11 \tLoss: tensor(1.5197, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 12 \tLoss: tensor(1.5399, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 13 \tLoss: tensor(1.5317, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 14 \tLoss: tensor(1.5379, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 15 \tLoss: tensor(1.5050, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 16 \tLoss: tensor(1.5196, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 17 \tLoss: tensor(1.5137, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 \tBatch: 18 \tLoss: tensor(1.5250, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 19 \tLoss: tensor(1.5110, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 20 \tLoss: tensor(1.5106, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 21 \tLoss: tensor(1.5247, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 22 \tLoss: tensor(1.5122, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 23 \tLoss: tensor(1.5413, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 24 \tLoss: tensor(1.5213, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 25 \tLoss: tensor(1.5034, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 26 \tLoss: tensor(1.5097, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 27 \tLoss: tensor(1.5147, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 28 \tLoss: tensor(1.5286, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 29 \tLoss: tensor(1.5350, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 30 \tLoss: tensor(1.5044, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 31 \tLoss: tensor(1.5251, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 32 \tLoss: tensor(1.5346, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 33 \tLoss: tensor(1.5318, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 34 \tLoss: tensor(1.5049, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 35 \tLoss: tensor(1.5150, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 36 \tLoss: tensor(1.5048, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 37 \tLoss: tensor(1.5057, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 38 \tLoss: tensor(1.5087, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 39 \tLoss: tensor(1.5086, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 40 \tLoss: tensor(1.5428, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 41 \tLoss: tensor(1.5157, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 42 \tLoss: tensor(1.5264, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 43 \tLoss: tensor(1.5174, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 44 \tLoss: tensor(1.4932, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 45 \tLoss: tensor(1.5232, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 46 \tLoss: tensor(1.5264, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 47 \tLoss: tensor(1.5263, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 48 \tLoss: tensor(1.5084, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 49 \tLoss: tensor(1.5163, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 50 \tLoss: tensor(1.5207, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 51 \tLoss: tensor(1.5321, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 52 \tLoss: tensor(1.5184, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 53 \tLoss: tensor(1.5280, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 54 \tLoss: tensor(1.5027, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 55 \tLoss: tensor(1.5302, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 56 \tLoss: tensor(1.5116, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 57 \tLoss: tensor(1.5115, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 58 \tLoss: tensor(1.5150, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 59 \tLoss: tensor(1.5130, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 60 \tLoss: tensor(1.5197, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 61 \tLoss: tensor(1.5295, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 62 \tLoss: tensor(1.5041, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 63 \tLoss: tensor(1.5038, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 64 \tLoss: tensor(1.5305, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 65 \tLoss: tensor(1.4993, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 66 \tLoss: tensor(1.4953, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 67 \tLoss: tensor(1.5048, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 68 \tLoss: tensor(1.5149, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 69 \tLoss: tensor(1.5215, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 70 \tLoss: tensor(1.5135, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 71 \tLoss: tensor(1.5246, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 72 \tLoss: tensor(1.5097, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 73 \tLoss: tensor(1.5024, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 74 \tLoss: tensor(1.4795, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 75 \tLoss: tensor(1.5202, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 76 \tLoss: tensor(1.5141, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 77 \tLoss: tensor(1.5305, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 78 \tLoss: tensor(1.5156, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 79 \tLoss: tensor(1.5167, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 80 \tLoss: tensor(1.5061, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 81 \tLoss: tensor(1.5067, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 82 \tLoss: tensor(1.5107, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 83 \tLoss: tensor(1.5392, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 84 \tLoss: tensor(1.5017, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 85 \tLoss: tensor(1.5239, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 86 \tLoss: tensor(1.5083, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 87 \tLoss: tensor(1.4998, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 88 \tLoss: tensor(1.5382, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 89 \tLoss: tensor(1.5227, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 90 \tLoss: tensor(1.5332, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 91 \tLoss: tensor(1.5294, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 92 \tLoss: tensor(1.5238, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 93 \tLoss: tensor(1.5058, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 94 \tLoss: tensor(1.5132, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 95 \tLoss: tensor(1.5254, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 96 \tLoss: tensor(1.5090, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 97 \tLoss: tensor(1.5011, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 98 \tLoss: tensor(1.4965, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 99 \tLoss: tensor(1.5119, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 100 \tLoss: tensor(1.5256, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 101 \tLoss: tensor(1.5090, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 102 \tLoss: tensor(1.5038, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 103 \tLoss: tensor(1.5080, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 104 \tLoss: tensor(1.5013, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 105 \tLoss: tensor(1.4933, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 106 \tLoss: tensor(1.5052, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 107 \tLoss: tensor(1.5066, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 108 \tLoss: tensor(1.5284, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 109 \tLoss: tensor(1.5114, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 110 \tLoss: tensor(1.5031, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 111 \tLoss: tensor(1.5328, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 112 \tLoss: tensor(1.5148, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 113 \tLoss: tensor(1.5023, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 114 \tLoss: tensor(1.5068, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 115 \tLoss: tensor(1.4919, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 116 \tLoss: tensor(1.5048, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 117 \tLoss: tensor(1.5297, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 118 \tLoss: tensor(1.5163, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 119 \tLoss: tensor(1.5292, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 120 \tLoss: tensor(1.5152, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 121 \tLoss: tensor(1.5349, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 122 \tLoss: tensor(1.5099, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 123 \tLoss: tensor(1.5161, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 124 \tLoss: tensor(1.5001, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 125 \tLoss: tensor(1.5179, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 126 \tLoss: tensor(1.5334, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 127 \tLoss: tensor(1.5044, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 128 \tLoss: tensor(1.5171, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 129 \tLoss: tensor(1.5195, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 130 \tLoss: tensor(1.5373, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 131 \tLoss: tensor(1.5246, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 132 \tLoss: tensor(1.5192, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 133 \tLoss: tensor(1.4978, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 134 \tLoss: tensor(1.5223, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 135 \tLoss: tensor(1.5137, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 136 \tLoss: tensor(1.5183, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 137 \tLoss: tensor(1.5239, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 138 \tLoss: tensor(1.5052, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 139 \tLoss: tensor(1.5224, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 \tBatch: 140 \tLoss: tensor(1.5677, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 141 \tLoss: tensor(1.4860, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 142 \tLoss: tensor(1.4953, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 143 \tLoss: tensor(1.5103, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 144 \tLoss: tensor(1.5129, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 145 \tLoss: tensor(1.4886, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 146 \tLoss: tensor(1.5097, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 147 \tLoss: tensor(1.5266, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 148 \tLoss: tensor(1.4940, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 149 \tLoss: tensor(1.5156, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 150 \tLoss: tensor(1.4860, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 151 \tLoss: tensor(1.5351, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 152 \tLoss: tensor(1.4993, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 153 \tLoss: tensor(1.5065, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 154 \tLoss: tensor(1.5233, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 155 \tLoss: tensor(1.5269, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 156 \tLoss: tensor(1.4965, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 157 \tLoss: tensor(1.5182, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 158 \tLoss: tensor(1.5260, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 159 \tLoss: tensor(1.5276, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 160 \tLoss: tensor(1.5186, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 161 \tLoss: tensor(1.5271, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 162 \tLoss: tensor(1.4960, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 163 \tLoss: tensor(1.5116, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 164 \tLoss: tensor(1.5003, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 165 \tLoss: tensor(1.4945, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 166 \tLoss: tensor(1.5221, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 167 \tLoss: tensor(1.5027, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 168 \tLoss: tensor(1.5448, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 169 \tLoss: tensor(1.5141, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 170 \tLoss: tensor(1.5118, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 171 \tLoss: tensor(1.5026, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 172 \tLoss: tensor(1.5120, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 173 \tLoss: tensor(1.5036, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 174 \tLoss: tensor(1.5244, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 175 \tLoss: tensor(1.4901, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 176 \tLoss: tensor(1.5077, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 177 \tLoss: tensor(1.5107, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 178 \tLoss: tensor(1.5200, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 179 \tLoss: tensor(1.5294, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 180 \tLoss: tensor(1.5123, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 181 \tLoss: tensor(1.5009, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 182 \tLoss: tensor(1.5160, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 183 \tLoss: tensor(1.5299, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 184 \tLoss: tensor(1.5062, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 185 \tLoss: tensor(1.5366, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 186 \tLoss: tensor(1.5263, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 187 \tLoss: tensor(1.5295, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 188 \tLoss: tensor(1.4983, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 189 \tLoss: tensor(1.5220, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 190 \tLoss: tensor(1.5082, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 191 \tLoss: tensor(1.4984, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 192 \tLoss: tensor(1.5116, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 193 \tLoss: tensor(1.5128, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 194 \tLoss: tensor(1.5055, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 195 \tLoss: tensor(1.4765, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 0 \tLoss: tensor(1.5136, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 1 \tLoss: tensor(1.5007, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 2 \tLoss: tensor(1.5039, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 3 \tLoss: tensor(1.5017, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 4 \tLoss: tensor(1.5042, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 5 \tLoss: tensor(1.4908, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 6 \tLoss: tensor(1.5008, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 7 \tLoss: tensor(1.5183, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 8 \tLoss: tensor(1.5127, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 9 \tLoss: tensor(1.5189, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 10 \tLoss: tensor(1.5180, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 11 \tLoss: tensor(1.5008, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 12 \tLoss: tensor(1.5289, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 13 \tLoss: tensor(1.4909, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 14 \tLoss: tensor(1.5354, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 15 \tLoss: tensor(1.5295, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 16 \tLoss: tensor(1.5218, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 17 \tLoss: tensor(1.5191, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 18 \tLoss: tensor(1.5306, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 19 \tLoss: tensor(1.5043, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 20 \tLoss: tensor(1.5133, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 21 \tLoss: tensor(1.5200, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 22 \tLoss: tensor(1.5035, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 23 \tLoss: tensor(1.5147, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 24 \tLoss: tensor(1.5189, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 25 \tLoss: tensor(1.5094, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 26 \tLoss: tensor(1.5089, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 27 \tLoss: tensor(1.5053, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 28 \tLoss: tensor(1.5000, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 29 \tLoss: tensor(1.5020, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 30 \tLoss: tensor(1.5130, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 31 \tLoss: tensor(1.5103, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 32 \tLoss: tensor(1.5251, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 33 \tLoss: tensor(1.4928, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 34 \tLoss: tensor(1.5170, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 35 \tLoss: tensor(1.5104, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 36 \tLoss: tensor(1.5067, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 37 \tLoss: tensor(1.5081, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 38 \tLoss: tensor(1.5098, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 39 \tLoss: tensor(1.4884, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 40 \tLoss: tensor(1.5026, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 41 \tLoss: tensor(1.5025, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 42 \tLoss: tensor(1.5219, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 43 \tLoss: tensor(1.5414, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 44 \tLoss: tensor(1.4974, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 45 \tLoss: tensor(1.4906, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 46 \tLoss: tensor(1.4918, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 47 \tLoss: tensor(1.5114, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 48 \tLoss: tensor(1.4942, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 49 \tLoss: tensor(1.5164, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 50 \tLoss: tensor(1.5213, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 51 \tLoss: tensor(1.5269, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 52 \tLoss: tensor(1.5186, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 53 \tLoss: tensor(1.5062, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 54 \tLoss: tensor(1.4975, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 55 \tLoss: tensor(1.4980, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 56 \tLoss: tensor(1.5169, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 57 \tLoss: tensor(1.4935, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 58 \tLoss: tensor(1.5186, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 59 \tLoss: tensor(1.5169, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 60 \tLoss: tensor(1.4944, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 61 \tLoss: tensor(1.5003, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 62 \tLoss: tensor(1.4993, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 63 \tLoss: tensor(1.4951, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 64 \tLoss: tensor(1.4915, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 65 \tLoss: tensor(1.5065, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 \tBatch: 66 \tLoss: tensor(1.5026, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 67 \tLoss: tensor(1.5157, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 68 \tLoss: tensor(1.4900, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 69 \tLoss: tensor(1.5069, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 70 \tLoss: tensor(1.5096, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 71 \tLoss: tensor(1.5268, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 72 \tLoss: tensor(1.5133, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 73 \tLoss: tensor(1.5086, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 74 \tLoss: tensor(1.5288, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 75 \tLoss: tensor(1.4961, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 76 \tLoss: tensor(1.4829, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 77 \tLoss: tensor(1.5205, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 78 \tLoss: tensor(1.5343, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 79 \tLoss: tensor(1.5066, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 80 \tLoss: tensor(1.5186, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 81 \tLoss: tensor(1.4929, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 82 \tLoss: tensor(1.5081, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 83 \tLoss: tensor(1.5452, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 84 \tLoss: tensor(1.5267, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 85 \tLoss: tensor(1.5227, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 86 \tLoss: tensor(1.4989, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 87 \tLoss: tensor(1.5134, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 88 \tLoss: tensor(1.5285, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 89 \tLoss: tensor(1.5165, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 90 \tLoss: tensor(1.5046, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 91 \tLoss: tensor(1.5212, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 92 \tLoss: tensor(1.4882, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 93 \tLoss: tensor(1.5032, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 94 \tLoss: tensor(1.4857, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 95 \tLoss: tensor(1.5306, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 96 \tLoss: tensor(1.5010, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 97 \tLoss: tensor(1.5231, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 98 \tLoss: tensor(1.5035, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 99 \tLoss: tensor(1.5112, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 100 \tLoss: tensor(1.5268, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 101 \tLoss: tensor(1.5419, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 102 \tLoss: tensor(1.4993, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 103 \tLoss: tensor(1.5307, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 104 \tLoss: tensor(1.5005, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 105 \tLoss: tensor(1.5100, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 106 \tLoss: tensor(1.5152, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 107 \tLoss: tensor(1.5137, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 108 \tLoss: tensor(1.5239, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 109 \tLoss: tensor(1.5045, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 110 \tLoss: tensor(1.5034, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 111 \tLoss: tensor(1.5024, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 112 \tLoss: tensor(1.5161, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 113 \tLoss: tensor(1.5202, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 114 \tLoss: tensor(1.5204, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 115 \tLoss: tensor(1.5270, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 116 \tLoss: tensor(1.4997, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 117 \tLoss: tensor(1.5207, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 118 \tLoss: tensor(1.5486, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 119 \tLoss: tensor(1.5229, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 120 \tLoss: tensor(1.5316, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 121 \tLoss: tensor(1.5102, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 122 \tLoss: tensor(1.5028, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 123 \tLoss: tensor(1.5279, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 124 \tLoss: tensor(1.5172, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 125 \tLoss: tensor(1.5165, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 126 \tLoss: tensor(1.5229, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 127 \tLoss: tensor(1.5132, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 128 \tLoss: tensor(1.5055, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 129 \tLoss: tensor(1.5139, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 130 \tLoss: tensor(1.4971, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 131 \tLoss: tensor(1.5525, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 132 \tLoss: tensor(1.5018, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 133 \tLoss: tensor(1.5020, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 134 \tLoss: tensor(1.5224, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 135 \tLoss: tensor(1.5166, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 136 \tLoss: tensor(1.4889, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 137 \tLoss: tensor(1.5003, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 138 \tLoss: tensor(1.5147, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 139 \tLoss: tensor(1.5057, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 140 \tLoss: tensor(1.5088, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 141 \tLoss: tensor(1.5644, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 142 \tLoss: tensor(1.5159, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 143 \tLoss: tensor(1.5354, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 144 \tLoss: tensor(1.5488, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 145 \tLoss: tensor(1.5287, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 146 \tLoss: tensor(1.5157, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 147 \tLoss: tensor(1.5132, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 148 \tLoss: tensor(1.5104, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 149 \tLoss: tensor(1.5272, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 150 \tLoss: tensor(1.4971, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 151 \tLoss: tensor(1.5071, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 152 \tLoss: tensor(1.4896, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 153 \tLoss: tensor(1.5204, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 154 \tLoss: tensor(1.4995, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 155 \tLoss: tensor(1.5130, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 156 \tLoss: tensor(1.4935, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 157 \tLoss: tensor(1.5125, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 158 \tLoss: tensor(1.5085, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 159 \tLoss: tensor(1.5025, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 160 \tLoss: tensor(1.5141, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 161 \tLoss: tensor(1.5098, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 162 \tLoss: tensor(1.5107, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 163 \tLoss: tensor(1.4977, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 164 \tLoss: tensor(1.4961, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 165 \tLoss: tensor(1.5112, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 166 \tLoss: tensor(1.5353, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 167 \tLoss: tensor(1.5142, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 168 \tLoss: tensor(1.4862, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 169 \tLoss: tensor(1.5094, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 170 \tLoss: tensor(1.5064, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 171 \tLoss: tensor(1.4942, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 172 \tLoss: tensor(1.5086, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 173 \tLoss: tensor(1.4924, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 174 \tLoss: tensor(1.4836, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 175 \tLoss: tensor(1.5173, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 176 \tLoss: tensor(1.5173, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 177 \tLoss: tensor(1.5126, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 178 \tLoss: tensor(1.5062, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 179 \tLoss: tensor(1.4976, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 180 \tLoss: tensor(1.5034, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 181 \tLoss: tensor(1.4989, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 182 \tLoss: tensor(1.5052, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 183 \tLoss: tensor(1.4992, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 184 \tLoss: tensor(1.5220, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 185 \tLoss: tensor(1.5212, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 186 \tLoss: tensor(1.5103, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 187 \tLoss: tensor(1.5268, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 \tBatch: 188 \tLoss: tensor(1.5151, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 189 \tLoss: tensor(1.5226, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 190 \tLoss: tensor(1.5007, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 191 \tLoss: tensor(1.5004, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 192 \tLoss: tensor(1.5241, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 193 \tLoss: tensor(1.5077, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 194 \tLoss: tensor(1.5047, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 195 \tLoss: tensor(1.5043, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 0 \tLoss: tensor(1.5077, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 1 \tLoss: tensor(1.5133, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 2 \tLoss: tensor(1.4999, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 3 \tLoss: tensor(1.4917, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 4 \tLoss: tensor(1.5118, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 5 \tLoss: tensor(1.5338, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 6 \tLoss: tensor(1.4973, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 7 \tLoss: tensor(1.4937, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 8 \tLoss: tensor(1.4973, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 9 \tLoss: tensor(1.4898, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 10 \tLoss: tensor(1.5077, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 11 \tLoss: tensor(1.5105, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 12 \tLoss: tensor(1.4868, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 13 \tLoss: tensor(1.5115, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 14 \tLoss: tensor(1.5135, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 15 \tLoss: tensor(1.5109, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 16 \tLoss: tensor(1.4902, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 17 \tLoss: tensor(1.5012, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 18 \tLoss: tensor(1.5080, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 19 \tLoss: tensor(1.4949, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 20 \tLoss: tensor(1.5193, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 21 \tLoss: tensor(1.5055, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 22 \tLoss: tensor(1.5040, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 23 \tLoss: tensor(1.5101, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 24 \tLoss: tensor(1.4917, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 25 \tLoss: tensor(1.4890, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 26 \tLoss: tensor(1.4867, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 27 \tLoss: tensor(1.4873, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 28 \tLoss: tensor(1.5012, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 29 \tLoss: tensor(1.4928, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 30 \tLoss: tensor(1.4972, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 31 \tLoss: tensor(1.5190, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 32 \tLoss: tensor(1.5381, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 33 \tLoss: tensor(1.5020, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 34 \tLoss: tensor(1.4768, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 35 \tLoss: tensor(1.4966, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 36 \tLoss: tensor(1.5126, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 37 \tLoss: tensor(1.5052, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 38 \tLoss: tensor(1.4954, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 39 \tLoss: tensor(1.4859, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 40 \tLoss: tensor(1.5012, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 41 \tLoss: tensor(1.4903, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 42 \tLoss: tensor(1.4873, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 43 \tLoss: tensor(1.5202, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 44 \tLoss: tensor(1.5050, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 45 \tLoss: tensor(1.4962, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 46 \tLoss: tensor(1.5062, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 47 \tLoss: tensor(1.5012, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 48 \tLoss: tensor(1.5074, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 49 \tLoss: tensor(1.5008, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 50 \tLoss: tensor(1.5015, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 51 \tLoss: tensor(1.5080, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 52 \tLoss: tensor(1.4857, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 53 \tLoss: tensor(1.4912, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 54 \tLoss: tensor(1.4903, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 55 \tLoss: tensor(1.4714, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 56 \tLoss: tensor(1.4911, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 57 \tLoss: tensor(1.4973, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 58 \tLoss: tensor(1.4978, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 59 \tLoss: tensor(1.4898, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 60 \tLoss: tensor(1.5051, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 61 \tLoss: tensor(1.5170, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 62 \tLoss: tensor(1.4982, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 63 \tLoss: tensor(1.5099, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 64 \tLoss: tensor(1.5041, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 65 \tLoss: tensor(1.5178, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 66 \tLoss: tensor(1.5218, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 67 \tLoss: tensor(1.4912, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 68 \tLoss: tensor(1.5054, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 69 \tLoss: tensor(1.4948, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 70 \tLoss: tensor(1.5005, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 71 \tLoss: tensor(1.5120, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 72 \tLoss: tensor(1.4881, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 73 \tLoss: tensor(1.4944, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 74 \tLoss: tensor(1.4904, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 75 \tLoss: tensor(1.4930, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 76 \tLoss: tensor(1.5258, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 77 \tLoss: tensor(1.5059, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 78 \tLoss: tensor(1.4937, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 79 \tLoss: tensor(1.4971, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 80 \tLoss: tensor(1.5135, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 81 \tLoss: tensor(1.5303, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 82 \tLoss: tensor(1.4928, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 83 \tLoss: tensor(1.5081, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 84 \tLoss: tensor(1.5037, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 85 \tLoss: tensor(1.5297, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 86 \tLoss: tensor(1.4955, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 87 \tLoss: tensor(1.4925, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 88 \tLoss: tensor(1.5106, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 89 \tLoss: tensor(1.5061, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 90 \tLoss: tensor(1.5048, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 91 \tLoss: tensor(1.5021, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 92 \tLoss: tensor(1.5157, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 93 \tLoss: tensor(1.4910, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 94 \tLoss: tensor(1.5069, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 95 \tLoss: tensor(1.5008, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 96 \tLoss: tensor(1.5081, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 97 \tLoss: tensor(1.4895, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 98 \tLoss: tensor(1.4894, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 99 \tLoss: tensor(1.4927, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 100 \tLoss: tensor(1.4929, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 101 \tLoss: tensor(1.4975, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 102 \tLoss: tensor(1.5144, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 103 \tLoss: tensor(1.5054, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 104 \tLoss: tensor(1.5091, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 105 \tLoss: tensor(1.5057, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 106 \tLoss: tensor(1.5146, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 107 \tLoss: tensor(1.4978, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 108 \tLoss: tensor(1.4905, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 109 \tLoss: tensor(1.5116, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 110 \tLoss: tensor(1.4941, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 111 \tLoss: tensor(1.5120, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 112 \tLoss: tensor(1.5152, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 113 \tLoss: tensor(1.5040, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 114 \tLoss: tensor(1.4945, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 115 \tLoss: tensor(1.4976, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 \tBatch: 116 \tLoss: tensor(1.4945, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 117 \tLoss: tensor(1.5127, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 118 \tLoss: tensor(1.4944, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 119 \tLoss: tensor(1.5140, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 120 \tLoss: tensor(1.4982, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 121 \tLoss: tensor(1.4900, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 122 \tLoss: tensor(1.4839, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 123 \tLoss: tensor(1.5126, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 124 \tLoss: tensor(1.5115, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 125 \tLoss: tensor(1.4853, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 126 \tLoss: tensor(1.4900, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 127 \tLoss: tensor(1.4850, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 128 \tLoss: tensor(1.4944, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 129 \tLoss: tensor(1.5018, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 130 \tLoss: tensor(1.5029, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 131 \tLoss: tensor(1.5036, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 132 \tLoss: tensor(1.5174, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 133 \tLoss: tensor(1.4990, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 134 \tLoss: tensor(1.4803, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 135 \tLoss: tensor(1.4988, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 136 \tLoss: tensor(1.5148, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 137 \tLoss: tensor(1.5025, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 138 \tLoss: tensor(1.4850, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 139 \tLoss: tensor(1.4924, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 140 \tLoss: tensor(1.5101, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 141 \tLoss: tensor(1.5063, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 142 \tLoss: tensor(1.5202, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 143 \tLoss: tensor(1.5008, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 144 \tLoss: tensor(1.4960, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 145 \tLoss: tensor(1.5063, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 146 \tLoss: tensor(1.4893, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 147 \tLoss: tensor(1.4984, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 148 \tLoss: tensor(1.4867, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 149 \tLoss: tensor(1.5233, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 150 \tLoss: tensor(1.4933, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 151 \tLoss: tensor(1.4889, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 152 \tLoss: tensor(1.5087, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 153 \tLoss: tensor(1.5214, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 154 \tLoss: tensor(1.4874, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 155 \tLoss: tensor(1.5055, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 156 \tLoss: tensor(1.4797, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 157 \tLoss: tensor(1.4956, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 158 \tLoss: tensor(1.5017, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 159 \tLoss: tensor(1.4946, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 160 \tLoss: tensor(1.4896, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 161 \tLoss: tensor(1.5134, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 162 \tLoss: tensor(1.5243, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 163 \tLoss: tensor(1.5036, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 164 \tLoss: tensor(1.5051, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 165 \tLoss: tensor(1.4829, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 166 \tLoss: tensor(1.4922, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 167 \tLoss: tensor(1.4861, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 168 \tLoss: tensor(1.4995, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 169 \tLoss: tensor(1.5027, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 170 \tLoss: tensor(1.5049, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 171 \tLoss: tensor(1.5129, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 172 \tLoss: tensor(1.5038, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 173 \tLoss: tensor(1.5111, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 174 \tLoss: tensor(1.5043, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 175 \tLoss: tensor(1.5002, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 176 \tLoss: tensor(1.5067, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 177 \tLoss: tensor(1.5115, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 178 \tLoss: tensor(1.5010, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 179 \tLoss: tensor(1.4993, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 180 \tLoss: tensor(1.4989, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 181 \tLoss: tensor(1.4972, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 182 \tLoss: tensor(1.5015, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 183 \tLoss: tensor(1.4924, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 184 \tLoss: tensor(1.4786, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 185 \tLoss: tensor(1.4959, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 186 \tLoss: tensor(1.5195, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 187 \tLoss: tensor(1.5014, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 188 \tLoss: tensor(1.4938, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 189 \tLoss: tensor(1.5206, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 190 \tLoss: tensor(1.4987, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 191 \tLoss: tensor(1.5146, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 192 \tLoss: tensor(1.4857, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 193 \tLoss: tensor(1.5017, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 194 \tLoss: tensor(1.5042, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 195 \tLoss: tensor(1.4958, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 0 \tLoss: tensor(1.4875, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 1 \tLoss: tensor(1.4901, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 2 \tLoss: tensor(1.4992, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 3 \tLoss: tensor(1.4954, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 4 \tLoss: tensor(1.5071, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 5 \tLoss: tensor(1.4948, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 6 \tLoss: tensor(1.4923, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 7 \tLoss: tensor(1.4929, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 8 \tLoss: tensor(1.5036, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 9 \tLoss: tensor(1.4941, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 10 \tLoss: tensor(1.4916, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 11 \tLoss: tensor(1.4955, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 12 \tLoss: tensor(1.4995, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 13 \tLoss: tensor(1.4809, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 14 \tLoss: tensor(1.4988, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 15 \tLoss: tensor(1.4784, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 16 \tLoss: tensor(1.5075, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 17 \tLoss: tensor(1.4886, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 18 \tLoss: tensor(1.4868, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 19 \tLoss: tensor(1.4859, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 20 \tLoss: tensor(1.4921, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 21 \tLoss: tensor(1.5022, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 22 \tLoss: tensor(1.4764, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 23 \tLoss: tensor(1.4962, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 24 \tLoss: tensor(1.4915, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 25 \tLoss: tensor(1.4958, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 26 \tLoss: tensor(1.4784, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 27 \tLoss: tensor(1.5091, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 28 \tLoss: tensor(1.4893, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 29 \tLoss: tensor(1.4898, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 30 \tLoss: tensor(1.4959, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 31 \tLoss: tensor(1.5174, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 32 \tLoss: tensor(1.4841, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 33 \tLoss: tensor(1.4993, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 34 \tLoss: tensor(1.4771, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 35 \tLoss: tensor(1.4877, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 36 \tLoss: tensor(1.4800, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 37 \tLoss: tensor(1.4926, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 38 \tLoss: tensor(1.5124, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 39 \tLoss: tensor(1.5005, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 \tBatch: 40 \tLoss: tensor(1.4983, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 41 \tLoss: tensor(1.5005, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 42 \tLoss: tensor(1.4944, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 43 \tLoss: tensor(1.5016, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 44 \tLoss: tensor(1.5282, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 45 \tLoss: tensor(1.4845, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 46 \tLoss: tensor(1.4811, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 47 \tLoss: tensor(1.5263, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 48 \tLoss: tensor(1.4985, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 49 \tLoss: tensor(1.4895, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 50 \tLoss: tensor(1.4974, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 51 \tLoss: tensor(1.4881, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 52 \tLoss: tensor(1.5017, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 53 \tLoss: tensor(1.4763, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 54 \tLoss: tensor(1.4938, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 55 \tLoss: tensor(1.5333, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 56 \tLoss: tensor(1.4862, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 57 \tLoss: tensor(1.5042, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 58 \tLoss: tensor(1.4775, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 59 \tLoss: tensor(1.5123, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 60 \tLoss: tensor(1.4969, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 61 \tLoss: tensor(1.4992, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 62 \tLoss: tensor(1.5015, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 63 \tLoss: tensor(1.4858, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 64 \tLoss: tensor(1.4884, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 65 \tLoss: tensor(1.4870, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 66 \tLoss: tensor(1.4884, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 67 \tLoss: tensor(1.4943, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 68 \tLoss: tensor(1.4912, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 69 \tLoss: tensor(1.4952, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 70 \tLoss: tensor(1.4952, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 71 \tLoss: tensor(1.5005, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 72 \tLoss: tensor(1.4766, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 73 \tLoss: tensor(1.5081, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 74 \tLoss: tensor(1.5087, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 75 \tLoss: tensor(1.5154, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 76 \tLoss: tensor(1.4993, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 77 \tLoss: tensor(1.5152, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 78 \tLoss: tensor(1.5063, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 79 \tLoss: tensor(1.4969, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 80 \tLoss: tensor(1.4961, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 81 \tLoss: tensor(1.4858, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 82 \tLoss: tensor(1.4921, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 83 \tLoss: tensor(1.4954, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 84 \tLoss: tensor(1.5140, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 85 \tLoss: tensor(1.4944, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 86 \tLoss: tensor(1.5083, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 87 \tLoss: tensor(1.5001, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 88 \tLoss: tensor(1.5181, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 89 \tLoss: tensor(1.4858, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 90 \tLoss: tensor(1.5090, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 91 \tLoss: tensor(1.4986, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 92 \tLoss: tensor(1.4870, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 93 \tLoss: tensor(1.5124, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 94 \tLoss: tensor(1.4967, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 95 \tLoss: tensor(1.5019, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 96 \tLoss: tensor(1.4974, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 97 \tLoss: tensor(1.4853, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 98 \tLoss: tensor(1.4938, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 99 \tLoss: tensor(1.4844, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 100 \tLoss: tensor(1.4878, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 101 \tLoss: tensor(1.4850, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 102 \tLoss: tensor(1.4961, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 103 \tLoss: tensor(1.5035, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 104 \tLoss: tensor(1.5092, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 105 \tLoss: tensor(1.4936, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 106 \tLoss: tensor(1.4993, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 107 \tLoss: tensor(1.4939, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 108 \tLoss: tensor(1.4981, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 109 \tLoss: tensor(1.4972, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 110 \tLoss: tensor(1.5034, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 111 \tLoss: tensor(1.4889, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 112 \tLoss: tensor(1.5069, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 113 \tLoss: tensor(1.5119, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 114 \tLoss: tensor(1.4959, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 115 \tLoss: tensor(1.4942, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 116 \tLoss: tensor(1.4873, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 117 \tLoss: tensor(1.5081, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 118 \tLoss: tensor(1.5071, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 119 \tLoss: tensor(1.4918, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 120 \tLoss: tensor(1.5005, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 121 \tLoss: tensor(1.5113, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 122 \tLoss: tensor(1.4839, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 123 \tLoss: tensor(1.4907, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 124 \tLoss: tensor(1.5188, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 125 \tLoss: tensor(1.4936, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 126 \tLoss: tensor(1.4808, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 127 \tLoss: tensor(1.4902, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 128 \tLoss: tensor(1.4858, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 129 \tLoss: tensor(1.5115, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 130 \tLoss: tensor(1.5116, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 131 \tLoss: tensor(1.4890, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 132 \tLoss: tensor(1.4870, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 133 \tLoss: tensor(1.4994, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 134 \tLoss: tensor(1.5091, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 135 \tLoss: tensor(1.5102, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 136 \tLoss: tensor(1.4973, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 137 \tLoss: tensor(1.5163, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 138 \tLoss: tensor(1.4824, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 139 \tLoss: tensor(1.4889, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 140 \tLoss: tensor(1.5047, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 141 \tLoss: tensor(1.5154, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 142 \tLoss: tensor(1.5005, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 143 \tLoss: tensor(1.5301, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 144 \tLoss: tensor(1.4803, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 145 \tLoss: tensor(1.4999, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 146 \tLoss: tensor(1.5148, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 147 \tLoss: tensor(1.4941, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 148 \tLoss: tensor(1.4852, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 149 \tLoss: tensor(1.4985, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 150 \tLoss: tensor(1.4869, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 151 \tLoss: tensor(1.4836, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 152 \tLoss: tensor(1.5129, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 153 \tLoss: tensor(1.4891, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 154 \tLoss: tensor(1.5018, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 155 \tLoss: tensor(1.5162, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 156 \tLoss: tensor(1.4897, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 157 \tLoss: tensor(1.5000, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 158 \tLoss: tensor(1.4873, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 \tBatch: 159 \tLoss: tensor(1.5017, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 160 \tLoss: tensor(1.4897, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 161 \tLoss: tensor(1.5050, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 162 \tLoss: tensor(1.4996, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 163 \tLoss: tensor(1.5124, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 164 \tLoss: tensor(1.4924, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 165 \tLoss: tensor(1.4955, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 166 \tLoss: tensor(1.5078, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 167 \tLoss: tensor(1.4813, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 168 \tLoss: tensor(1.4912, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 169 \tLoss: tensor(1.4911, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 170 \tLoss: tensor(1.4981, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 171 \tLoss: tensor(1.5071, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 172 \tLoss: tensor(1.4992, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 173 \tLoss: tensor(1.5038, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 174 \tLoss: tensor(1.4851, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 175 \tLoss: tensor(1.5095, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 176 \tLoss: tensor(1.4798, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 177 \tLoss: tensor(1.5355, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 178 \tLoss: tensor(1.4897, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 179 \tLoss: tensor(1.4987, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 180 \tLoss: tensor(1.4978, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 181 \tLoss: tensor(1.4891, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 182 \tLoss: tensor(1.4788, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 183 \tLoss: tensor(1.5002, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 184 \tLoss: tensor(1.5024, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 185 \tLoss: tensor(1.4992, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 186 \tLoss: tensor(1.4935, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 187 \tLoss: tensor(1.5047, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 188 \tLoss: tensor(1.4881, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 189 \tLoss: tensor(1.5063, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 190 \tLoss: tensor(1.5082, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 191 \tLoss: tensor(1.4900, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 192 \tLoss: tensor(1.4851, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 193 \tLoss: tensor(1.5143, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 194 \tLoss: tensor(1.4892, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 195 \tLoss: tensor(1.4920, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 0 \tLoss: tensor(1.4839, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 1 \tLoss: tensor(1.5091, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 2 \tLoss: tensor(1.4940, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 3 \tLoss: tensor(1.5078, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 4 \tLoss: tensor(1.4775, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 5 \tLoss: tensor(1.5021, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 6 \tLoss: tensor(1.5090, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 7 \tLoss: tensor(1.4815, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 8 \tLoss: tensor(1.5102, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 9 \tLoss: tensor(1.5013, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 10 \tLoss: tensor(1.4959, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 11 \tLoss: tensor(1.5082, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 12 \tLoss: tensor(1.5024, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 13 \tLoss: tensor(1.4988, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 14 \tLoss: tensor(1.4929, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 15 \tLoss: tensor(1.4923, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 16 \tLoss: tensor(1.4963, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 17 \tLoss: tensor(1.4862, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 18 \tLoss: tensor(1.5024, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 19 \tLoss: tensor(1.4734, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 20 \tLoss: tensor(1.4897, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 21 \tLoss: tensor(1.4880, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 22 \tLoss: tensor(1.4877, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 23 \tLoss: tensor(1.5098, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 24 \tLoss: tensor(1.4988, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 25 \tLoss: tensor(1.4883, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 26 \tLoss: tensor(1.5141, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 27 \tLoss: tensor(1.4742, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 28 \tLoss: tensor(1.4881, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 29 \tLoss: tensor(1.4996, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 30 \tLoss: tensor(1.4923, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 31 \tLoss: tensor(1.4880, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 32 \tLoss: tensor(1.4900, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 33 \tLoss: tensor(1.4970, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 34 \tLoss: tensor(1.4863, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 35 \tLoss: tensor(1.4757, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 36 \tLoss: tensor(1.4935, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 37 \tLoss: tensor(1.5133, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 38 \tLoss: tensor(1.4962, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 39 \tLoss: tensor(1.5125, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 40 \tLoss: tensor(1.5059, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 41 \tLoss: tensor(1.4929, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 42 \tLoss: tensor(1.5076, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 43 \tLoss: tensor(1.4710, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 44 \tLoss: tensor(1.4825, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 45 \tLoss: tensor(1.5120, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 46 \tLoss: tensor(1.4890, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 47 \tLoss: tensor(1.4855, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 48 \tLoss: tensor(1.5021, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 49 \tLoss: tensor(1.4975, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 50 \tLoss: tensor(1.4884, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 51 \tLoss: tensor(1.4872, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 52 \tLoss: tensor(1.4994, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 53 \tLoss: tensor(1.4914, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 54 \tLoss: tensor(1.4754, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 55 \tLoss: tensor(1.4837, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 56 \tLoss: tensor(1.4945, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 57 \tLoss: tensor(1.5128, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 58 \tLoss: tensor(1.4781, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 59 \tLoss: tensor(1.5042, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 60 \tLoss: tensor(1.5015, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 61 \tLoss: tensor(1.4970, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 62 \tLoss: tensor(1.4961, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 63 \tLoss: tensor(1.4994, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 64 \tLoss: tensor(1.4876, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 65 \tLoss: tensor(1.4798, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 66 \tLoss: tensor(1.4939, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 67 \tLoss: tensor(1.4810, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 68 \tLoss: tensor(1.4811, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 69 \tLoss: tensor(1.4893, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 70 \tLoss: tensor(1.4924, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 71 \tLoss: tensor(1.4801, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 72 \tLoss: tensor(1.4887, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 73 \tLoss: tensor(1.4814, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 74 \tLoss: tensor(1.4810, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 75 \tLoss: tensor(1.5099, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 76 \tLoss: tensor(1.4915, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 77 \tLoss: tensor(1.5057, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 78 \tLoss: tensor(1.4856, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 79 \tLoss: tensor(1.4895, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 80 \tLoss: tensor(1.4859, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 81 \tLoss: tensor(1.5013, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 82 \tLoss: tensor(1.5012, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 83 \tLoss: tensor(1.4981, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 84 \tLoss: tensor(1.4932, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 85 \tLoss: tensor(1.5069, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 \tBatch: 86 \tLoss: tensor(1.4943, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 87 \tLoss: tensor(1.4872, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 88 \tLoss: tensor(1.4806, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 89 \tLoss: tensor(1.5012, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 90 \tLoss: tensor(1.4962, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 91 \tLoss: tensor(1.4779, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 92 \tLoss: tensor(1.4883, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 93 \tLoss: tensor(1.5118, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 94 \tLoss: tensor(1.4990, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 95 \tLoss: tensor(1.5104, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 96 \tLoss: tensor(1.4969, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 97 \tLoss: tensor(1.4936, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 98 \tLoss: tensor(1.4920, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 99 \tLoss: tensor(1.4892, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 100 \tLoss: tensor(1.5117, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 101 \tLoss: tensor(1.4936, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 102 \tLoss: tensor(1.5024, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 103 \tLoss: tensor(1.5007, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 104 \tLoss: tensor(1.4801, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 105 \tLoss: tensor(1.4920, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 106 \tLoss: tensor(1.5063, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 107 \tLoss: tensor(1.4852, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 108 \tLoss: tensor(1.5041, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 109 \tLoss: tensor(1.4898, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 110 \tLoss: tensor(1.4942, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 111 \tLoss: tensor(1.4928, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 112 \tLoss: tensor(1.5091, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 113 \tLoss: tensor(1.4931, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 114 \tLoss: tensor(1.4983, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 115 \tLoss: tensor(1.4869, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 116 \tLoss: tensor(1.4834, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 117 \tLoss: tensor(1.4890, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 118 \tLoss: tensor(1.4862, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 119 \tLoss: tensor(1.4947, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 120 \tLoss: tensor(1.4950, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 121 \tLoss: tensor(1.4866, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 122 \tLoss: tensor(1.4972, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 123 \tLoss: tensor(1.5018, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 124 \tLoss: tensor(1.5015, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 125 \tLoss: tensor(1.4930, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 126 \tLoss: tensor(1.4853, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 127 \tLoss: tensor(1.5045, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 128 \tLoss: tensor(1.4908, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 129 \tLoss: tensor(1.4942, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 130 \tLoss: tensor(1.4904, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 131 \tLoss: tensor(1.4857, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 132 \tLoss: tensor(1.5306, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 133 \tLoss: tensor(1.4975, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 134 \tLoss: tensor(1.4916, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 135 \tLoss: tensor(1.4901, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 136 \tLoss: tensor(1.5145, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 137 \tLoss: tensor(1.4870, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 138 \tLoss: tensor(1.4945, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 139 \tLoss: tensor(1.5075, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 140 \tLoss: tensor(1.5064, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 141 \tLoss: tensor(1.4902, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 142 \tLoss: tensor(1.5049, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 143 \tLoss: tensor(1.4940, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 144 \tLoss: tensor(1.4997, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 145 \tLoss: tensor(1.4838, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 146 \tLoss: tensor(1.5042, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 147 \tLoss: tensor(1.5161, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 148 \tLoss: tensor(1.4912, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 149 \tLoss: tensor(1.4943, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 150 \tLoss: tensor(1.5120, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 151 \tLoss: tensor(1.5066, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 152 \tLoss: tensor(1.4980, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 153 \tLoss: tensor(1.4925, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 154 \tLoss: tensor(1.5050, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 155 \tLoss: tensor(1.4913, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 156 \tLoss: tensor(1.5070, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 157 \tLoss: tensor(1.4938, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 158 \tLoss: tensor(1.4955, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 159 \tLoss: tensor(1.5018, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 160 \tLoss: tensor(1.4893, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 161 \tLoss: tensor(1.4832, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 162 \tLoss: tensor(1.5019, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 163 \tLoss: tensor(1.4894, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 164 \tLoss: tensor(1.4936, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 165 \tLoss: tensor(1.4856, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 166 \tLoss: tensor(1.5042, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 167 \tLoss: tensor(1.4939, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 168 \tLoss: tensor(1.4809, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 169 \tLoss: tensor(1.4993, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 170 \tLoss: tensor(1.4992, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 171 \tLoss: tensor(1.4771, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 172 \tLoss: tensor(1.4951, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 173 \tLoss: tensor(1.4968, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 174 \tLoss: tensor(1.5012, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 175 \tLoss: tensor(1.4850, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 176 \tLoss: tensor(1.5019, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 177 \tLoss: tensor(1.5101, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 178 \tLoss: tensor(1.4971, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 179 \tLoss: tensor(1.4883, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 180 \tLoss: tensor(1.5046, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 181 \tLoss: tensor(1.4971, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 182 \tLoss: tensor(1.4847, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 183 \tLoss: tensor(1.4914, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 184 \tLoss: tensor(1.4910, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 185 \tLoss: tensor(1.4814, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 186 \tLoss: tensor(1.4963, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 187 \tLoss: tensor(1.4951, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 188 \tLoss: tensor(1.4794, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 189 \tLoss: tensor(1.5022, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 190 \tLoss: tensor(1.4928, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 191 \tLoss: tensor(1.4897, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 192 \tLoss: tensor(1.4934, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 193 \tLoss: tensor(1.4752, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 194 \tLoss: tensor(1.4958, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 195 \tLoss: tensor(1.5024, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "my_nn = OGNN(originators=2) # initialise our model\n",
    "\n",
    "# CREATE OUR OPTIMISER\n",
    "optimiser = torch.optim.Adam(              # what optimiser should we use?\n",
    "    my_nn.parameters(),          # what should it optimise?\n",
    ")\n",
    "        \n",
    "# CREATE OUR CRITERION\n",
    "criterion = torch.nn.CrossEntropyLoss() # returns a callable object that compares our predictions to our labels and returns our loss\n",
    "\n",
    "# SET UP TRAINING VISUALISATION\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter() # we will use this to show our models performance on a graph\n",
    "    \n",
    "# TRAINING LOOP\n",
    "def train(model, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for idx, minibatch in enumerate(train_loader): # for each mini-batch sampled from the training dataloader\n",
    "            inputs, labels = minibatch # unpack the inputs and labels from the minibatch\n",
    "            prediction = model(inputs) # pass the data forward through the model\n",
    "            reg_loss = 100*torch.sum(torch.Tensor([F.mse_loss(my_nn.l1_originators[0].weight, my_nn.l1_originators[1].weight), F.mse_loss(my_nn.l2_originators[0].weight, my_nn.l2_originators[1].weight), F.mse_loss(my_nn.l3_originators[0].weight, my_nn.l3_originators[1].weight)]))# compute the loss\n",
    "            loss = criterion(prediction, labels)# compute the loss\n",
    "            total_loss = reg_loss+loss\n",
    "            print('Epoch:', epoch, '\\tBatch:', idx, '\\tLoss:', loss)\n",
    "            optimiser.zero_grad() # reset the gradients attribute of each of the model's params to zero\n",
    "            total_loss.backward() # backward pass to compute and set all of the model param's gradients\n",
    "            optimiser.step() # update the model's parameters\n",
    "            writer.add_scalar('Loss/Train', loss, epoch*len(train_loader) + idx) # write loss to a graph\n",
    "            \n",
    "            \n",
    "train(my_nn, 8) # train for 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.61\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "            \n",
    "def test(model):\n",
    "    num_correct = 0\n",
    "    num_examples = len(test_data) # test DATA not test LOADER\n",
    "    for inputs, labels in test_loader: # for all exampls, over all mini-batches in the test dataset\n",
    "        predictions = model(inputs) # make prediction\n",
    "        predictions = torch.max(predictions, axis=1) # reduce to find max indices along direction which column varies\n",
    "        predictions = predictions[1] # torch.max returns (values, indices)\n",
    "        num_correct += int(sum(predictions == labels))\n",
    "    percent_correct = num_correct / num_examples * 100 # compute percentage\n",
    "    print('Accuracy:', percent_correct)\n",
    "    \n",
    "test(my_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.0023, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0013, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0020, grad_fn=<MeanBackward0>)]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diffs_interlayer = [\n",
    "    F.mse_loss(my_nn.l1_originators[0].weight, my_nn.l1_originators[1].weight),\n",
    "    F.mse_loss(my_nn.l2_originators[0].weight, my_nn.l2_originators[1].weight),\n",
    "    F.mse_loss(my_nn.l3_originators[0].weight, my_nn.l3_originators[1].weight)\n",
    "]\n",
    "diffs\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haron/.local/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: Using a target size (torch.Size([256, 1024])) that is different to the input size (torch.Size([1024, 784])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (784) must match the size of tensor b (1024) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-edb8e0cda8a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m diffs_interlayer = [\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#F.mse_loss(list(my_nn.parameters())[2], list(my_nn.parameters())[4]),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#F.mse_loss(list(my_nn.parameters())[0], list(my_nn.parameters())[4])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2197\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2198\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2199\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'none'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2201\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'mean'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (784) must match the size of tensor b (1024) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "diffs_interlayer = [\n",
    "    np.mean([F.mse_loss(list(my_nn.parameters())[0], list(my_nn.parameters())[4]), F.mse_loss(list(my_nn.parameters())[0], list(my_nn.parameters())[6]), F.mse_loss(list(my_nn.parameters())[2], list(my_nn.parameters())[4]), F.mse_loss(list(my_nn.parameters())[2], list(my_nn.parameters())[6])]),\n",
    "    np.mean([F.mse_loss(list(my_nn.parameters())[4], list(my_nn.parameters())[8]), F.mse_loss(list(my_nn.parameters())[4], list(my_nn.parameters())[10]), F.mse_loss(list(my_nn.parameters())[6], list(my_nn.parameters())[8]), F.mse_loss(list(my_nn.parameters())[6], list(my_nn.parameters())[10])]),\n",
    "    #F.mse_loss(list(my_nn.parameters())[2], list(my_nn.parameters())[4]),\n",
    "    #F.mse_loss(list(my_nn.parameters())[0], list(my_nn.parameters())[4])\n",
    "    #F.mse_loss(list(my_nn.parameters())[4], list(my_nn.parameters())[6]),\n",
    "    #F.mse_loss(list(my_nn.parameters())[8], list(my_nn.parameters())[10]),\n",
    "]\n",
    "diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.0023, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0013, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0020, grad_fn=<MeanBackward0>)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_nn.state_dict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
